@article{Gomez-Uribe2015,
abstract = {This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.},
author = {Gomez-Uribe, Carlos A. and Hunt, Neil},
isbn = {2158-656X},
issn = {2158656X},
journal = {ACM Transactions on Management Information Systems},
number = {4},
pages = {1--19},
title = {{The Netflix Recommender System}},
volume = {6},
year = {2015}
}

@misc{Richardson2006,
author = {Richardson, Daniel},
publisher = {Department of Computer Science, University of Bath.},
title = {{Formal systems , logic and semantics}},
url = {http://www.cs.bath.ac.uk/pb/EMCL/DS/DS-Ref-2011/c19.pdf},
year = {2006}
}

@book{Nilsson1982,
  Author = {Nils J. Nilsson},
  Title = {Principles of Artificial Intelligence (Symbolic Computation)},
  Publisher = {Springer},
  Year = {1982}
}

@article{Campbell2002,
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
author = {Campbell, Murray and {Hoane Jr.}, a. Joseph and Hsu, Feng-hsiung},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {computer chess,evaluation,game tree search,parallel search,search extensions,selective search},
number = {1-2},
pages = {57--83},
title = {{Deep Blue}},
url = {http://www.sciencedirect.com/science/article/pii/S0004370201001291},
volume = {134},
year = {2002}
}

@article{Wood1990,
abstract = {Includes indexes.},
author = {Wood, M.McGee},
isbn = {0937073180 (pbk.)$\backslash$r0937073172 (hard)},
issn = {01676423},
journal = {Science of Computer Programming},
number = {1},
pages = {110},
title = {{Prolog and natural-language analysis}},
volume = {14},
year = {1990}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Chapelle2011,
abstract = {Thompson sampling is one of oldest heuristic to address the exploration ex- ploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
author = {Chapelle, Olivier and Li, Lihong},
journal = {Advances in Neural Information Processing Systems},
pages = {2249----2257},
title = {{An Empirical Evaluation of Thompson Sampling}},
url = {http://explo.cs.ucl.ac.uk/wp-content/uploads/2011/05/An-Empirical-Evaluation-of-Thompson-Sampling-Chapelle-Li-2011.pdf},
year = {2011}
}

@Article{Watkins1992,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698"
}

@article{Berges1995,
author = {Berges, Vincent-pierre and Pryzant, Reid},
pages = {1--8},
title = {{Reinforcement Learning for Atari Breakout}},
year = {1995}
}

@article{silver2017,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
year = {2017},
month = {10},
pages = {354-359},
title = {Mastering the game of Go without human knowledge},
volume = {550},
booktitle = {Nature}
}

@article{Sabour2017,
author = {Sabour, Sara and Nov, C V and Hinton, Geoffrey E},
eprint = {arXiv:1710.09829v2},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
year = {2017}
}
